{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "RqlNbSMuRF1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "!pip install -U sentence-transformers\n",
        "!pip install razdel\n",
        "!pip install natasha\n",
        "!pip install striprtf\n",
        "!pip install diff_match_patch"
      ],
      "metadata": {
        "id": "hiPC_JZFSDLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from razdel import tokenize, sentenize\n",
        "from natasha import *\n",
        "\n",
        "from docx import Document\n",
        "from striprtf.striprtf import rtf_to_text\n",
        "\n",
        "# import aspose.words as aw\n",
        "import difflib as dl\n",
        "import diff_match_patch as dmp_module\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification,AutoModel\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch"
      ],
      "metadata": {
        "id": "6jPkp7OlSFeF"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1_path = 'vkr1.docx'\n",
        "doc2_path = 'vkr2.docx'"
      ],
      "metadata": {
        "id": "4W3op0EYRFNd"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1_path = 'short1.docx'\n",
        "doc2_path = 'short2.docx'"
      ],
      "metadata": {
        "id": "fmmYUAJk2snT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1_path ='short1_rtf.rtf'\n",
        "doc2_path ='short2_rtf.rtf'"
      ],
      "metadata": {
        "id": "OILuzh2pCdhI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"surdan/LaBSE_ner_nerel\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"surdan/LaBSE_ner_nerel\")\n",
        "nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"first\")\n",
        "model_sim = SentenceTransformer('uaritm/multilingual_en_ru_uk')\n",
        "\n",
        "label_impotance={\n",
        "    \"MONEY\" : 1,\n",
        "    \"ORGANIZATION\" : 0.5,\n",
        "    \"PERSON\" : 0.5,\n",
        "    \"FACILITY\" : 0.5\n",
        "}"
      ],
      "metadata": {
        "id": "cfzKKWzAuUIF"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_text(filename: str) -> dict:\n",
        "    '''\n",
        "    считывание документа и разбиение его на предложения\n",
        "    с использованием Natasha\n",
        "    возвращает словарь с номерами строк и их содержанием\n",
        "    '''\n",
        "    full_txt = {}\n",
        "    line_number = 0\n",
        "    if pathlib.Path(filename).suffix == '.docx':\n",
        "        doc = Document(filename)\n",
        "        for i,para in enumerate(doc.paragraphs):\n",
        "            for sen in list(sentenize(para.text)):\n",
        "                full_txt[line_number] = (sen.text,i)\n",
        "                line_number+=1\n",
        "    else: # работа с форматом rtf\n",
        "        with open(filename, 'r') as file:\n",
        "            text = file.read()\n",
        "        \n",
        "        rtf = text\n",
        "        text1 = rtf_to_text(rtf)\n",
        "\n",
        "        for sen in list(sentenize(text1)):\n",
        "            full_txt[line_number] = sen.text\n",
        "            line_number+=1\n",
        "\n",
        "    return {key: val for key,val in full_txt.items() if val[0] != ''}"
      ],
      "metadata": {
        "id": "-I7ZfnDKXg7v"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sent_similarity_quickly(query: str, passage: list):\n",
        "    '''\n",
        "    вычислений меры \"похожести\" предложений\n",
        "    '''\n",
        "    scores = []\n",
        "    for i,(sen,_) in passage:\n",
        "        scores.append(dl.SequenceMatcher(lambda x: x == \" \",query,sen).ratio())\n",
        "    if max(scores) > 0.65:\n",
        "        return passage[np.argmax(np.asarray(scores), axis=0)][0]\n",
        "    return False"
      ],
      "metadata": {
        "id": "eTtLIQEaFl2Z"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_match(t1: list, t2: list):\n",
        "    '''\n",
        "    соотнесение измененных и одинаковых на основе\n",
        "    прямого сравнивания и меры похожести, основанной\n",
        "    на косинусном расстоянии между векторами эмбедингов\n",
        "    '''\n",
        "    t1 = t1.copy()\n",
        "    t2 = t2.copy()\n",
        "    d_eq = {}\n",
        "    d_changed = {}\n",
        "    scrap = '',' ','-','.','  ',' .'\n",
        "    for raw1_k,raw1_v in t1.items():\n",
        "        not_found = True\n",
        "        for raw2_k, raw2_v in t2.items():\n",
        "            if raw1_v[0]==raw2_v[0] and raw2_v[0]!='' and raw2_v[0]!='':\n",
        "                d_eq[raw2_k] = raw1_k\n",
        "                del t2[raw2_k]\n",
        "                not_found = False\n",
        "                break\n",
        "\n",
        "        if not_found and raw1_v[0] not in scrap:\n",
        "            l_values = list(t2.items())\n",
        "            sim = get_sent_similarity_quickly(raw1_v[0],l_values)\n",
        "            if sim:\n",
        "                d_changed[raw1_k] = sim\n",
        "    \n",
        "    return d_eq, dict((v,k) for k,v in d_changed.items())\n",
        "    # t1 - values | t2 - keys"
      ],
      "metadata": {
        "id": "z3D7CCnQuQy4"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minus_and_plus(t1,t2,d_eq,d_changed):\n",
        "    '''\n",
        "    вычисление пересечений\n",
        "    на выходе удаленные предложения из 1 текста\n",
        "    и добавленные во 2-й\n",
        "    '''\n",
        "    deleted = set(t1.keys())-set.union(set(d_eq.values()), set(d_changed.values())) # удалено из t1\n",
        "    added = set(t2)-set.union(set(d_eq.keys()), set(d_changed.keys())) # добавлено в t2\n",
        "    return deleted, added"
      ],
      "metadata": {
        "id": "PuGXDy5RClfa"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentance_diff(sen1: str, sen2: str) -> list:\n",
        "    '''\n",
        "    принимает на вход 2 строки (предложения), вычисляется разница\n",
        "    на выходе получается список (-1, 0, 19, 'Алгоритм Дойче-Йозе')\n",
        "    где [0]:\n",
        "    \"-\" - измменение или удаление части текста\n",
        "    \"0\" - отсутствие изменений\n",
        "    \"1\" - добавление нового фрагмента\n",
        "    [1]:[2] соответствующий фрагмент текста\n",
        "    [3] текст (сущность) \n",
        "    '''\n",
        "    dmp = dmp_module.diff_match_patch()\n",
        "    diff = dmp.diff_main(sen1, sen2)\n",
        "    dmp.diff_cleanupSemantic(diff)\n",
        "    counter = 0\n",
        "    out = []\n",
        "    sent = ''\n",
        "    for el in diff:\n",
        "        out.append((el[0],counter,counter+len(el[1]),el[1]))\n",
        "        counter+=len(el[1])\n",
        "        sennt+=el[1]\n",
        "    return out, sent"
      ],
      "metadata": {
        "id": "Qg643vXyWleL"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_diff_to_html(sen1, sen2):\n",
        "    '''\n",
        "    создание HTML размметки по предложениям\n",
        "    на основе внесенных изменений\n",
        "    '''\n",
        "    dmp = diff_match_patch()\n",
        "    diff = dmp.diff_main(sen1, sen2)\n",
        "    dmp.diff_cleanupSemantic(diff)\n",
        "    \n",
        "    return dmp.diff_prettyHtml(diff)"
      ],
      "metadata": {
        "id": "ljd_z94KqIXJ"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sent_similarity(query, passage):\n",
        "    '''\n",
        "    вычисление коэффициента \"похожести\" предложений используя similarity BERT\n",
        "    принимает строку основного предложения и список из предложений или строку\n",
        "    возвращает коэф. похожести на основное предложение\n",
        "    '''\n",
        "    query_embedding = model_sim.encode(query)\n",
        "    passage_embedding = model_sim.encode(passage)\n",
        "    score = util.cos_sim(query_embedding, passage_embedding)\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "1SJrQW_F7Nj7"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tag_diff_score(doc,doc1,label_impotance={}):\n",
        "  '''\n",
        "  Принимает словари из get_ner_tokens() для 2-х сматченных предложений\n",
        "  Возращает оценку, основанную на пересечении токенов\n",
        "  Можно передать словарь весов токенов для учета похожести слов внутри одной категории\n",
        "   - в этом случае будет учитыватся степень сходства каждого слова (0,1) умноженное на коэффициент из словаря\n",
        "   - коэффициенты из словаря должны быть от 0 до 1\n",
        "   - по умолчанию коэффициенты для отсутсвующих тегов равны 0\n",
        "  '''\n",
        "\n",
        "  c=0\n",
        "  words=[]\n",
        "  for i in doc['ents']:\n",
        "    for j in doc1['ents']: \n",
        "      if i['word']==j['word']:\n",
        "        words.append(i['word'])\n",
        "        c+=1\n",
        "\n",
        "\n",
        "  l1=[i for i in doc['ents'] if i['word'] not in words]\n",
        "  l2=[i for i in doc1['ents'] if i['word'] not in words]\n",
        "\n",
        "  numerator=0\n",
        "  denumerator=0\n",
        "\n",
        "  for i in l1:\n",
        "    max_num=0\n",
        "    for j in l2:\n",
        "      if i['label']==j['label']:\n",
        "        cos = dl.SequenceMatcher(lambda x: x == \"\",i['word'],j['word']).ratio()\n",
        "        max_num = cos if cos>max_num else max_num\n",
        "    numerator+= max_num * (0 if not (i['label'] in label_impotance.keys()) else label_impotance[i['label']])\n",
        "\n",
        "    \n",
        "    denumerator+=1 \n",
        "\n",
        "  for i in l2:\n",
        "    max_num=0\n",
        "    for j in l1:\n",
        "      if i['label']==j['label']:\n",
        "        cos = dl.SequenceMatcher(lambda x: x == \"\",i['word'],j['word']).ratio()\n",
        "        max_num = cos if cos>max_num else max_num\n",
        "    numerator+=max_num * (0 if not (i['label'] in label_impotance.keys()) else label_impotance[i['label']])\n",
        "    denumerator+=1\n",
        "\n",
        "  return 0 if denumerator+c==0 else (c+numerator)/(denumerator+c)"
      ],
      "metadata": {
        "id": "wlxAQw45qtYs"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ner_tokens(text, nlp, conf_for_bert=0.8, label_drop_list=[]):\n",
        "  '''\n",
        "  Возращает словарь из текста предложения и токенов в нем\n",
        "  На вход кушает текст и nlp из huggingface\n",
        "  '''\n",
        "\n",
        "  def rubert_to_lst_dict(rubert):\n",
        "    r=list(rubert.copy())\n",
        "    for i in r:\n",
        "      i['label'] = i['entity_group']\n",
        "      i['score'] =float(i['score'])\n",
        "    return r\n",
        "\n",
        "\n",
        "  def cut_conf_less_than(dct,conf):\n",
        "    return [i for i in dct if i['score']>conf]\n",
        "\n",
        "\n",
        "  def cut_tag(dct,lst):\n",
        "    return [i for i in dct if (i['label'] not in lst)]\n",
        "\n",
        "\n",
        "  ans=cut_tag(cut_conf_less_than(rubert_to_lst_dict(nlp(text)),conf_for_bert),label_drop_list) \n",
        "  ans={    \n",
        "      'text' : text,\n",
        "      'ents' : ans\n",
        "      }\n",
        "\n",
        "  return ans\n"
      ],
      "metadata": {
        "id": "jbbIpNvNt-38"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entity_extract(sentanse: str) ->dict:\n",
        "    '''\n",
        "    выделение сущностей из предложения\n",
        "    '''\n",
        "    # morph_vocab = MorphVocab()\n",
        "    # names_extractor = NamesExtractor(morph_vocab)\n",
        "    # dates_extractor = DatesExtractor(morph_vocab)\n",
        "    # money_extractor = MoneyExtractor(morph_vocab)\n",
        "    # addr_extractor = AddrExtractor(morph_vocab)\n",
        "\n",
        "\n",
        "    # extractors=[dates_extractor,money_extractor,addr_extractor]\n",
        "\n",
        "    # doc=get_ner_tokens(sentanse,tokenizer,model_ent,nlp,extractors,conf_for_bert=0.7,label_drop_list=['LOC'])\n",
        "    doc=get_ner_tokens(sentanse,nlp,conf_for_bert=0.7,label_drop_list=[])\n",
        "\n",
        "    return doc"
      ],
      "metadata": {
        "id": "G8bXJI6KKIHl"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_json(t1,t2,d_eq,d_changed,deleted):\n",
        "    '''\n",
        "    формирование файла разметки.\n",
        "\n",
        "    '''\n",
        "    out = []\n",
        "    d_finaly = {}\n",
        "\n",
        "    for k,v in t2.items():\n",
        "        d_finaly = {}\n",
        "        d_finaly[\"id\"] = k\n",
        "        d_finaly[\"text\"] = v[0]\n",
        "        d_finaly[\"num_paragraph\"] = v[1]\n",
        "\n",
        "        if d_eq.get(k) is not None:\n",
        "            d_finaly[\"score\"] = 1\n",
        "            d_finaly[\"n_matches\"] = d_eq[k]\n",
        "            d_finaly[\"importance\"] = 0\n",
        "        elif d_changed.get(k) is not None:\n",
        "            tags_1 = entity_extract(t1[d_changed[k]][0])\n",
        "            tags_2 = entity_extract(v[0])\n",
        "            d_finaly[\"sim_score\"] = round(get_sent_similarity(v[0],t1[d_changed[k]][0]).item(),3)\n",
        "            d_finaly[\"entity_score\"] = get_tag_diff_score(tags_1,tags_2,label_impotance={})\n",
        "            d_finaly[\"check_test\"] = t1[d_changed[k]]\n",
        "            d_finaly[\"check_test_entities\"] = tags_1['ents']\n",
        "            d_finaly[\"n_matches\"] = d_changed[k]\n",
        "            d_finaly['entities'] = tags_2['ents']\n",
        "            d_finaly[\"markdown\"] = get_diff_to_html(t1[d_changed[k]][0], v[0]) # разметка для двух текстов\n",
        "            d_finaly[\"importance\"] = 2 # 1-5\n",
        "\n",
        "        else: # добавленные предложения\n",
        "            d_finaly[\"score\"] = 0\n",
        "            d_finaly[\"n_matches\"] = False\n",
        "            d_finaly[\"entities\"] = entity_extract(v[0])['ents']\n",
        "            d_finaly[\"importance\"] = 1\n",
        "        \n",
        "        out.append(d_finaly)\n",
        "    \n",
        "    d_finaly = {}\n",
        "    d_finaly['eq_and_match'] = out\n",
        "\n",
        "    del_out = {}\n",
        "    del_lst = []\n",
        "    for d in deleted:\n",
        "        del_out[\"id\"] = d\n",
        "        del_out[\"text\"] = t1[d][0]\n",
        "        del_out[\"num_paragraph\"] = t1[d][1]\n",
        "        del_out['score'] = 0\n",
        "        del_out[\"n_matches\"] = False\n",
        "        del_out[\"entities\"] = entity_extract(t1[d][0])['ents']\n",
        "        del_out[\"importance\"] = 1\n",
        "\n",
        "        del_lst.append(del_out)\n",
        "\n",
        "    d_finaly['deleted'] = del_lst\n",
        "\n",
        "\n",
        "    with open('markdown.json', 'w') as outfile:\n",
        "        json.dump(d_finaly, outfile, ensure_ascii=False)\n",
        "\n",
        "    with open('text1.json', 'w') as outfile:\n",
        "        json.dump(t1, outfile, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "27dKa-ocH1qR"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1_path = 'v1.docx'\n",
        "doc2_path = 'v2.docx'"
      ],
      "metadata": {
        "id": "spqNILkF_ohN"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "t1 = get_all_text(doc1_path)\n",
        "t2 = get_all_text(doc2_path)\n",
        "d_eq, d_changed = get_match(t1,t2)\n",
        "deleted, added = get_minus_and_plus(t1,t2,d_eq,d_changed)\n",
        "get_json(t1,t2,d_eq,d_changed,deleted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQupoilyYSlf",
        "outputId": "c26b296f-2b70-4f9e-ea52-5a0a36e06547"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min, sys: 215 ms, total: 1min 1s\n",
            "Wall time: 1min 2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# t1 = get_all_text(doc1_path)\n",
        "# t2 = get_all_text(doc2_path)\n",
        "# d_eq, d_changed = get_match(t1,t2)\n",
        "# deleted, added = get_minus_and_plus(t1,t2,d_eq,d_changed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agZGuJMpC0om",
        "outputId": "b5a84a5b-08ce-4534-a5d2-07b8653eb529"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16.4 s, sys: 29.3 ms, total: 16.4 s\n",
            "Wall time: 16.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Тестирование"
      ],
      "metadata": {
        "id": "idzMuZkv004E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# added_sent, del_sent = get_diff(markdown)"
      ],
      "metadata": {
        "id": "MKp85p5o1Keu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_sent_similarity_quickly(query, passage):\n",
        "#     scores = []\n",
        "#     for sen in passage:\n",
        "#         scores.append(dl.SequenceMatcher(lambda x: x == \" \",query,sen).ratio())\n",
        "#     if max(scores) > 0.85:\n",
        "#         return np.argmax(np.asarray(scores), axis=0)\n",
        "#     return -1\n",
        "#     # return  "
      ],
      "metadata": {
        "id": "EfY4cR2_bsNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def pred_moderate(pred):\n",
        "#     '''\n",
        "#     проверка на прохождение порога соответствия\n",
        "#     возвращяет индекс сматченного предложения\n",
        "#     '''\n",
        "    \n",
        "#     if torch.max(pred) > 0.9:\n",
        "#         return torch.argmax(pred, dim=1).item()\n",
        "\n",
        "#     return 0\n",
        "    "
      ],
      "metadata": {
        "id": "l821N-baCSPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for k,v in matched_sent.items():\n",
        "#     print(f'{markdown[k]}')\n",
        "#     print(f'{markdown[v]}')\n",
        "#     print(f'--------------')"
      ],
      "metadata": {
        "id": "hrVNABG8b2nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = 'Алгоритм Дойче-Йозе подходит для решения задач регрессии за 5000 рублей лет и классификации Валерой Бабушкиным, которому исполнилось тридцать лет 30 февраля на пр. Боголюбова, 17, Дубна, Московская обл., 141981'\n",
        "text2 = 'Завод ООО \"Рога и копыта\" подходит для решения задач регрессии за 5000 рублей и классификации Валерой Рубчанов, которому исполнилось тридцать лет 30 сентября на пр. Боголюбова, 17, Дубна, 141981'"
      ],
      "metadata": {
        "id": "p-JnvGVV8iXI"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_extract('Я вчера купил ДУБНУ и МАРС')"
      ],
      "metadata": {
        "id": "Ybk-OcbHzP3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b5ca515-a229-477b-ca99-5a42eeea4d56"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'Я вчера купил ДУБНУ и МАРС',\n",
              " 'ents': [{'entity_group': 'DATE',\n",
              "   'score': 0.9750596880912781,\n",
              "   'word': 'вчера',\n",
              "   'start': 2,\n",
              "   'end': 7,\n",
              "   'label': 'DATE'},\n",
              "  {'entity_group': 'EVENT',\n",
              "   'score': 0.9971312284469604,\n",
              "   'word': 'купил',\n",
              "   'start': 8,\n",
              "   'end': 13,\n",
              "   'label': 'EVENT'},\n",
              "  {'entity_group': 'PRODUCT',\n",
              "   'score': 0.9980424642562866,\n",
              "   'word': 'ДУБНУ',\n",
              "   'start': 14,\n",
              "   'end': 19,\n",
              "   'label': 'PRODUCT'},\n",
              "  {'entity_group': 'PRODUCT',\n",
              "   'score': 0.9940086603164673,\n",
              "   'word': 'МАРС',\n",
              "   'start': 22,\n",
              "   'end': 26,\n",
              "   'label': 'PRODUCT'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw = 'Особая экономическая зона должна принять новых резидентов в этом году'\n",
        "changed = 'Особая экономическая зона не должна принять новых резидентов в этом году'\n",
        "dl.SequenceMatcher(lambda x: x == \"\",raw,changed).ratio()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqrAVZxVVWks",
        "outputId": "026e2597-7b68-4f92-ac81-2440d78cc0cb"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9787234042553191"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    }
  ]
}